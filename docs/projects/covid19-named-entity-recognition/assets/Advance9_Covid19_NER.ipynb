{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advance Approach 9 for Covid19 NER\n",
    "\n",
    "Utilize an embedding layer and the bidirectional GRU with optimizer Adam.\n",
    "\n",
    "Testing with the size of the embedding layer and the size of the bidirectional GRU layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys in train_dict: dict_keys(['id', 'word_seq', 'tag_seq'])\n",
      "keys in val_dict: dict_keys(['id', 'word_seq', 'tag_seq'])\n",
      "keys in test_dict: dict_keys(['id', 'word_seq'])\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle as pkl\n",
    "\n",
    "train_dict = pkl.load(open(\"data/train.pkl\", \"rb\"))\n",
    "val_dict = pkl.load(open(\"data/val.pkl\", \"rb\"))\n",
    "test_dict = pkl.load(open(\"data/test.pkl\", \"rb\"))\n",
    "print(\"keys in train_dict:\", train_dict.keys())\n",
    "print(\"keys in val_dict:\", val_dict.keys())\n",
    "print(\"keys in test_dict:\", test_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an entry of the dataset\n",
    "#print(\"index:\", train_dict[\"id\"][0])\n",
    "#print(*zip(train_dict[\"word_seq\"][0], train_dict[\"tag_seq\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the NER tags:\n",
      " 0: LABORATORY_OR_TEST_RESULT\n",
      " 1: VIRAL_PROTEIN\n",
      " 2: PHYSICAL_SCIENCE\n",
      " 3: _t_pad_\n",
      " 4: FAC\n",
      " 5: INJURY_OR_POISONING\n",
      " 6: GPE\n",
      " 7: TIME\n",
      " 8: PRODUCT\n",
      " 9: ORGANISM\n",
      "10: O\n",
      "11: MATERIAL\n",
      "12: WILDLIFE\n",
      "13: GROUP_ATTRIBUTE\n",
      "14: LABORATORY_PROCEDURE\n",
      "15: MOLECULAR_FUNCTION\n",
      "16: INDIVIDUAL_BEHAVIOR\n",
      "17: ARCHAEON\n",
      "18: LIVESTOCK\n",
      "19: MACHINE_ACTIVITY\n",
      "20: SIGN_OR_SYMPTOM\n",
      "21: GENE_OR_GENOME\n",
      "22: CARDINAL\n",
      "23: SUBSTRATE\n",
      "24: QUANTITY\n",
      "25: BODY_SUBSTANCE\n",
      "26: LOC\n",
      "27: RESEARCH_ACTIVITY\n",
      "28: SOCIAL_BEHAVIOR\n",
      "29: HUMAN-CAUSED_PHENOMENON_OR_PROCESS\n",
      "30: BODY_PART_ORGAN_OR_ORGAN_COMPONENT\n",
      "31: DIAGNOSTIC_PROCEDURE\n",
      "32: CELL_COMPONENT\n",
      "33: DATE\n",
      "34: VIRUS\n",
      "35: ORGAN_OR_TISSUE_FUNCTION\n",
      "36: FOOD\n",
      "37: IMMUNE_RESPONSE\n",
      "38: ORG\n",
      "39: LAW\n",
      "40: CELL\n",
      "41: GROUP\n",
      "42: LANGUAGE\n",
      "43: TISSUE\n",
      "44: EUKARYOTE\n",
      "45: BACTERIUM\n",
      "46: ORDINAL\n",
      "47: CHEMICAL\n",
      "48: EXPERIMENTAL_MODEL_OF_DISEASE\n",
      "49: PERSON\n",
      "50: THERAPEUTIC_OR_PREVENTIVE_PROCEDURE\n",
      "51: WORK_OF_ART\n",
      "52: CORONAVIRUS\n",
      "53: DISEASE_OR_SYNDROME\n",
      "54: CELL_OR_MOLECULAR_DYSFUNCTION\n",
      "55: GOVERNMENTAL_OR_REGULATORY_ACTIVITY\n",
      "56: EVENT\n",
      "57: EVOLUTION\n",
      "58: MONEY\n",
      "59: CELL_FUNCTION\n",
      "60: EDUCATIONAL_ACTIVITY\n",
      "61: ANATOMICAL_STRUCTURE\n",
      "62: DAILY_OR_RECREATIONAL_ACTIVITY\n",
      "63: NORP\n",
      "64: PERCENT\n"
     ]
    }
   ],
   "source": [
    "# all the NER tags:\n",
    "from itertools import chain\n",
    "#print(\"count of the NER tags:\", len(set(chain(*train_dict[\"tag_seq\"]))))\n",
    "#print(\"all the NER tags:\", set(chain(*train_dict[\"tag_seq\"])))\n",
    "tag_set = set(chain(*train_dict[\"tag_seq\"]));\n",
    "print(\"All the NER tags:\")\n",
    "for idx, tag in zip(range(len(tag_set)), tag_set):\n",
    "    print(\"{:2d}: {}\".format(idx, tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of word vocab: 82275 size of tag_dict: 65\n"
     ]
    }
   ],
   "source": [
    "# prepare word vocab and tag vocab\n",
    "\n",
    "vocab_dict = {'_unk_': 0, '_w_pad_': 1}\n",
    "\n",
    "for doc in train_dict['word_seq']:\n",
    "    for word in doc:\n",
    "        if(word not in vocab_dict):\n",
    "            vocab_dict[word] = len(vocab_dict)\n",
    "\n",
    "tag_dict = {'_t_pad_': 0} # add a padding token\n",
    "\n",
    "for tag_seq in train_dict['tag_seq']:\n",
    "    for tag in tag_seq:\n",
    "        if(tag not in tag_dict):\n",
    "            tag_dict[tag] = len(tag_dict)\n",
    "word2idx = vocab_dict\n",
    "idx2word = {v:k for k,v in word2idx.items()}\n",
    "tag2idx = tag_dict\n",
    "idx2tag = {v:k for k,v in tag2idx.items()}\n",
    "\n",
    "print(\"size of word vocab:\", len(vocab_dict), \"size of tag_dict:\", len(tag_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum length of a sentence is set to 128\n",
    "max_sent_length = 128\n",
    "\n",
    "train_tokens = np.array([[word2idx[w] for w in doc] for doc in train_dict['word_seq']])\n",
    "val_tokens = np.array([[word2idx.get(w, 0) for w in doc] for doc in val_dict['word_seq']])\n",
    "test_tokens = np.array([[word2idx.get(w, 0) for w in doc] for doc in test_dict['word_seq']])\n",
    "\n",
    "\n",
    "train_tags = [[tag2idx[t] for t in t_seq] for t_seq in train_dict['tag_seq']]\n",
    "#train_tags = np.array(train_tags)\n",
    "train_tags = np.array([to_categorical(t_seq, num_classes=len(tag_dict)) for t_seq in train_tags])\n",
    "\n",
    "val_tags = [[tag2idx[t] for t in t_seq] for t_seq in val_dict['tag_seq']]\n",
    "#val_tags = np.array(val_tags)\n",
    "val_tags = np.array([to_categorical(t_seq, num_classes=len(tag_dict)) for t_seq in val_tags])\n",
    "\n",
    "# we don't have test tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training size: (23600, 128) tag size: (23600, 128, 65)\n",
      "validating size: (2950, 128) tag size: (2950, 128, 65)\n"
     ]
    }
   ],
   "source": [
    "print(\"training size:\", train_tokens.shape, \"tag size:\", train_tags.shape)\n",
    "print(\"validating size:\", val_tokens.shape, \"tag size:\", val_tags.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2  3  4  5  6  7  8  9 10 11] [1 1 2 1 1 3 3 1 4 4]\n"
     ]
    }
   ],
   "source": [
    "# an example of training instance and training tags.\n",
    "#print(train_tokens[0,:10], train_tags[0, :10])\n",
    "print(train_tokens[0,:10], np.argmax(train_tags[0, :10, :], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "       19, 20, 17, 18, 21, 22, 23, 24, 25, 12, 13, 14, 26, 27, 28, 25, 29,\n",
       "       19, 13, 30, 31, 21, 32, 33, 34, 35, 14, 35, 36, 33, 37, 23, 38, 39,\n",
       "       21, 40, 33, 14, 35, 36, 33, 37, 23, 41, 42, 43, 44, 45, 46, 47, 42,\n",
       "       48, 44, 49, 50, 22, 51, 52, 53, 54, 55, 56, 57, 51, 58, 59, 60, 14,\n",
       "       26, 61, 62, 63, 64, 65, 38, 66, 67, 14, 26, 19, 20, 68,  3, 69, 70,\n",
       "       71, 38, 70, 72,  4, 73, 74, 61, 75, 76, 77, 24, 78, 79, 80, 10, 11,\n",
       "       81, 22, 82, 83, 84, 85,  9, 86, 31])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tags[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Embedding, Dropout, BatchNormalization, Input, Add, Concatenate,\\\n",
    "    Bidirectional, SimpleRNN, LSTM, GRU, TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_RNN(input_length, vocab_size, embedding_size,\n",
    "              hidden_size, output_size, num_tags,\n",
    "              num_rnn_layers, num_mlp_layers,\n",
    "              rnn_type=\"lstm\",\n",
    "              bidirectional=False,\n",
    "              activation=\"tanh\",\n",
    "              dropout_rate=0.0,\n",
    "              batch_norm=False,\n",
    "              l2_reg=0.0,\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"Adam\",\n",
    "              learning_rate=0.001,\n",
    "              metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    :param input_length: the maximum length of sentences, type: int\n",
    "    :param vocab_size: the vacabulary size, type: int\n",
    "    :param embedding_size: the dimension of word representations, type: int\n",
    "    :param hidden_size: the dimension of the hidden states, type: int\n",
    "    :param output_size: the dimension of the prediction, type: int\n",
    "    :param num_tags: the number of tag types, type: int\n",
    "    :param num_rnn_layers: the number of layers of the RNN, type: int\n",
    "    :param num_mlp_layers: the number of layers of the MLP, type: int\n",
    "    :param rnn_type: the type of RNN, type: str\n",
    "    :param bidirectional: whether to use bidirectional rnn, type: bool\n",
    "    :param activation: the activation type, type: str\n",
    "    :param dropout_rate: the probability of dropout, type: float\n",
    "    :param batch_norm: whether to enable batch normalization, type: bool\n",
    "    :param l2_reg: the weight for the L2 regularizer, type: str\n",
    "    :param loss: the training loss, type: str\n",
    "    :param optimizer: the optimizer, type: str\n",
    "    :param learning_rate: the learning rate for the optimizer, type: float\n",
    "    :param metric: the metric, type: str\n",
    "    return a RNN for NER with num_tags tag types,\n",
    "    # activation document: https://keras.io/activations/\n",
    "    # dropout document: https://keras.io/layers/core/#dropout\n",
    "    # embedding document: https://keras.io/layers/embeddings/#embedding\n",
    "    # recurrent layers document: https://keras.io/layers/recurrent\n",
    "    # batch normalization document: https://keras.io/layers/normalization/\n",
    "    # losses document: https://keras.io/losses/\n",
    "    # optimizers document: https://keras.io/optimizers/\n",
    "    # metrics document: https://keras.io/metrics/\n",
    "    \"\"\"\n",
    "    x = Input(shape=(input_length,))\n",
    "    \n",
    "    ################################\n",
    "    ###### Word Representation #####\n",
    "    ################################\n",
    "    # word representation layer\n",
    "    emb = Embedding(input_dim=vocab_size,\n",
    "                    output_dim=embedding_size,\n",
    "                    input_length=input_length,\n",
    "                    embeddings_initializer=keras.initializers.TruncatedNormal(mean=0.0, stddev=0.1, seed=0))(x)\n",
    "    \n",
    "    ################################\n",
    "    ####### Recurrent Layers #######\n",
    "    ################################\n",
    "    # recurrent layers\n",
    "    # Referennce: https://keras.io/api/layers/#recurrent-layers\n",
    "    if rnn_type == \"rnn\":\n",
    "        fn = SimpleRNN\n",
    "    elif rnn_type == \"lstm\":\n",
    "        fn = LSTM\n",
    "    elif rnn_type == \"gru\":\n",
    "        fn = GRU\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    h = emb\n",
    "    for i in range(num_rnn_layers):\n",
    "        #is_last = (i == num_rnn_layers-1)\n",
    "        if bidirectional:\n",
    "            h = Bidirectional(fn(hidden_size,\n",
    "                   kernel_initializer=keras.initializers.glorot_uniform(seed=0),\n",
    "                   recurrent_initializer=keras.initializers.Orthogonal(gain=1.0, seed=0),\n",
    "                   return_sequences=True))(h)\n",
    "            # return_sequences:\n",
    "            # Boolean. Whether to return the last output. in the output sequence, or the full sequence.\n",
    "            # [h_1, h_2, ..., h_n] or h_n\n",
    "        else:\n",
    "            h = fn(hidden_size,\n",
    "                   kernel_initializer=keras.initializers.glorot_uniform(seed=0),\n",
    "                   recurrent_initializer=keras.initializers.Orthogonal(gain=1.0, seed=0),\n",
    "                   return_sequences=True)(h)\n",
    "        h = Dropout(dropout_rate, seed=0)(h)\n",
    "    \n",
    "    ################################\n",
    "    #### Fully Connected Layers ####\n",
    "    ################################\n",
    "    # multi-layer perceptron\n",
    "    for i in range(num_mlp_layers-1):\n",
    "        new_h = Dense(hidden_size,\n",
    "                      kernel_initializer=keras.initializers.he_normal(seed=0),\n",
    "                      bias_initializer=\"zeros\",\n",
    "                      kernel_regularizer=keras.regularizers.l2(l2_reg))(h)\n",
    "        # add batch normalization layer\n",
    "        if batch_norm:\n",
    "            new_h = BatchNormalization()(new_h)\n",
    "        # add residual connection\n",
    "        if i == 0:\n",
    "            h = new_h\n",
    "        else:\n",
    "            h = Add()([h, new_h])\n",
    "        # add activation\n",
    "        h = Activation(activation)(h)\n",
    "    y = TimeDistributed(Dense(num_tags, activation='softmax'))(h)\n",
    "    #y = Dense(output_size,\n",
    "    #          activation=\"softmax\",\n",
    "    #          kernel_initializer=keras.initializers.he_normal(seed=0),\n",
    "    #          bias_initializer=\"zeros\")(h)\n",
    "    \n",
    "    # set the loss, the optimizer, and the metric\n",
    "    if optimizer == \"SGD\":\n",
    "        optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    elif optimizer == \"RMSprop\":\n",
    "        optmizer = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer == \"Adam\":\n",
    "        optmizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    model = Model(x, y)\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=[metric])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "import tensorflow as tf\n",
    "seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"models\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 500\n",
    "hidden_size = max_sent_length*2\n",
    "num_rnn_layers = 1\n",
    "num_mlp_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_RNN(max_sent_length, len(vocab_dict), embedding_size,\n",
    "              hidden_size, max_sent_length, len(tag_dict),\n",
    "              num_rnn_layers, num_mlp_layers,\n",
    "              rnn_type=\"gru\",\n",
    "              bidirectional=True,\n",
    "              activation=\"tanh\",\n",
    "              dropout_rate=0.0,\n",
    "              batch_norm=False,\n",
    "              l2_reg=0.0,\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"Adam\",\n",
    "              learning_rate=0.001,\n",
    "              metric=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 128, 500)          41137500  \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 128, 512)          1164288   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128, 512)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 128, 65)           33345     \n",
      "=================================================================\n",
      "Total params: 42,335,133\n",
      "Trainable params: 42,335,133\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23600, 128)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23600, 128, 65)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tags.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.config.run_functions_eagerly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "213/213 [==============================] - 496s 2s/step - loss: 0.8122 - accuracy: 0.8219 - val_loss: 0.4621 - val_accuracy: 0.8758\n",
      "Epoch 2/10\n",
      "213/213 [==============================] - 502s 2s/step - loss: 0.3779 - accuracy: 0.8939 - val_loss: 0.3643 - val_accuracy: 0.8978\n",
      "Epoch 3/10\n",
      "213/213 [==============================] - 526s 2s/step - loss: 0.2832 - accuracy: 0.9159 - val_loss: 0.3365 - val_accuracy: 0.9048\n",
      "Epoch 4/10\n",
      "213/213 [==============================] - 505s 2s/step - loss: 0.2301 - accuracy: 0.9296 - val_loss: 0.3302 - val_accuracy: 0.9091\n",
      "Epoch 5/10\n",
      "213/213 [==============================] - 511s 2s/step - loss: 0.1932 - accuracy: 0.9398 - val_loss: 0.3290 - val_accuracy: 0.9113\n",
      "Epoch 6/10\n",
      "213/213 [==============================] - 513s 2s/step - loss: 0.1639 - accuracy: 0.9486 - val_loss: 0.3364 - val_accuracy: 0.9113\n",
      "Epoch 7/10\n",
      "213/213 [==============================] - 522s 2s/step - loss: 0.1383 - accuracy: 0.9565 - val_loss: 0.3474 - val_accuracy: 0.9134\n",
      "Epoch 8/10\n",
      "213/213 [==============================] - 532s 2s/step - loss: 0.1152 - accuracy: 0.9636 - val_loss: 0.3642 - val_accuracy: 0.9147\n",
      "Epoch 9/10\n",
      "213/213 [==============================] - 529s 2s/step - loss: 0.0955 - accuracy: 0.9699 - val_loss: 0.3845 - val_accuracy: 0.9144\n",
      "Epoch 10/10\n",
      "213/213 [==============================] - 509s 2s/step - loss: 0.0773 - accuracy: 0.9758 - val_loss: 0.4088 - val_accuracy: 0.9118\n"
     ]
    }
   ],
   "source": [
    "checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(\"models\", \"bi_gru_emb500_hidden2unit.hdf5\"),\n",
    "    monitor=\"val_accuracy\",\n",
    "    verbose=0,\n",
    "    save_best_only=True)\n",
    "earlystopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    verbose=0)\n",
    "\n",
    "rnn_history = model.fit(train_tokens, train_tags,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=10, batch_size=100, verbose=1,\n",
    "                    callbacks=[checkpointer, earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236/236 [==============================] - 111s 471ms/step - loss: 0.1154 - accuracy: 0.9674\n",
      "30/30 [==============================] - 14s 454ms/step - loss: 0.3745 - accuracy: 0.9129\n",
      "training loss: 0.11543704569339752 training accuracy 0.9673775434494019\n",
      "test loss: 0.37453693151474 test accuracy 0.9129475355148315\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(os.path.join(\"models\", \"bi_gru_emb500_hidden2unit.hdf5\"))\n",
    "train_score = model.evaluate(train_tokens, train_tags,\n",
    "                             batch_size=100)\n",
    "test_score = model.evaluate(val_tokens, val_tags,\n",
    "                            batch_size=100)\n",
    "print(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\n",
    "print(\"test loss:\", test_score[0], \"test accuracy\", test_score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = model.predict(val_tokens, batch_size=10)\n",
    "val_preds = np.argmax(val_preds, axis=2)\n",
    "val_preds = [[idx2tag[idx] for idx in sentence] for sentence in val_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2950"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2950"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = model.predict(test_tokens, batch_size=10)\n",
    "test_preds = np.argmax(test_preds, axis=2)\n",
    "test_preds = [[idx2tag[idx] for idx in sentence] for sentence in test_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2950"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2950"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_predictions(ids, predictions,fileName):\n",
    "    df = pd.DataFrame({'id': ids,\n",
    "                   'labels': [json.dumps(np.array(preds).tolist()) for preds in predictions]})\n",
    "    df.to_csv(fileName, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_predictions(val_dict[\"id\"], val_preds, 'val_preds.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_predictions(test_dict[\"id\"], test_preds, 'test_preds.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provided function to test accuracy\n",
    "# You could check the validation accuracy to select the best of your models\n",
    "def calc_accuracy(preds, tags, padding_id=\"_t_pad_\"):\n",
    "    \"\"\"\n",
    "        Input:\n",
    "            preds (np.narray): (num_data, length_sentence)\n",
    "            tags  (np.narray): (num_data, length_sentence)\n",
    "        Output:\n",
    "            Proportion of correct prediction. The padding tokens are filtered out.\n",
    "    \"\"\"\n",
    "    preds_flatten = preds.flatten()\n",
    "    tags_flatten = tags.flatten()\n",
    "    non_padding_idx = np.where(tags_flatten!=padding_id)[0]\n",
    "    \n",
    "    return sum(preds_flatten[non_padding_idx]==tags_flatten[non_padding_idx])/len(non_padding_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(pred_file, ground_file):\n",
    "    file_dict = pkl.load(open(ground_file, \"rb\"))\n",
    "    file_preds = pd.read_csv(pred_file)\n",
    "    return calc_accuracy(np.array([json.loads(line) for line in file_preds[\"labels\"]]), \n",
    "              np.array(file_dict[\"tag_seq\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9049119152998352"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(\"val_preds.csv\", \"data/val.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv",
   "language": "python",
   "name": "local-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
